
![mingpt](mingpt.jpg)

# minGPT: Подсчет вхождений символа

Решение задачи по курсу "Language generative modeling course" М. Гончарова на базовой кафедре ИППИ РАН 4 курс. Репозиторий содержит саму библиотеку `mingpt`, готовый конфиг и утилиты для обучения, валидации и запуска инференса из консоли.

**Условие**

Подсчет вхождений символа в строку

**Вход**

строка из символов (без пробелов), пробел, один символ

**Выход**

сколько раз символ встречается в строке

**Пример**

*Вход*: aabbaac a  
*Выход*: 4

## Быстрый старт

- **Python 3.10+** и torch ≥ 2.0. Удобно работать в отдельном виртуальном окружении (`python -m venv .venv`).
- Из корня репозитория активируйте окружение и установите зависимости:

	```bash
	source .venv/bin/activate
	pip install --upgrade pip
	pip install -r requirements.txt
	# при необходимости переопределите индекс в requirements.txt под свою сборку torch
	pip install -e .
	```

- `requirements.txt` фиксирует версии `numpy`, `matplotlib`, wheel `torch==2.5.1+cu121` и ссылку на библиотеку `mingpt`; для CPU можно заменить колёсико Torch на версию с официального PyPI.

- Для проверки окружения можно выполнить unit-тест из каталога `tests/`:

	```bash
	python -m unittest discover tests
	```

## Обучение модели

**Запустить сразу все:**
```bash
bash run.sh
```

- Основной сценарий запускается так:

	```bash
	python train.py config.json
	```

- Гиперпараметры и пути задаются в `config.json` (по умолчанию веса сохраняются в `out/countchar/model.pt`).
- После завершения в каталоге `out/countchar/` появятся сохранённые веса (`model.pt`), график (`curves.png`) и копии параметров запуска (`args.txt`, `config.json`).

## Проверка и инференс

- Быстрая валидация на 20 случайных примерах:

	```bash
	python val.py config.json
	```

	На текущем чекпойнте (`out/countchar/model.pt`) точность составила **19/20 (95%)**.

- Запуск inference для произвольной строки ≤ `max_len`:

	```bash
	python test.py "smssssssssssss s"
	# → 13
	```

## Результаты

- Архитектура `gpt-mini` (~2.7M параметров) обучается ~6000 итераций на синтетических данных со сбалансированным распределением целевых значений.
- Полученное качество: 95% точности на случайной 20-примерной выборке, валидируемой скриптом `val.py`.

## Особенности реализации

- Переменная длина примеров: `L` случайна на [1, `max_len`], что устраняет зависимость от фиксированной длины.
- Формат ввода/вывода: `"<строка> <символ>|" → "<число>~"` с EOS-токеном и маскированием паддинга на потере.
- Балансировка: число вхождений `k` выбирается равновероятно, выборки train/val/test разделены по хэшу пары `(s, c)`.
- Сохранение артефактов обучения с периодической оценкой `val_loss` и `accuracy`, записываемых на график.

## Структура репозитория

- `mingpt/` — минимальная библиотека GPT (модель, BPE, Trainer, утилиты).
- `train.py`, `val.py`, `test.py` — скрипты для обучения, оценки и инференса в задаче подсчёта символов.
- `projects/` — дополнительные демонстрационные проекты (`adder`, `chargpt`).
- `demo.ipynb`, `generate.ipynb` — ноутбуки с примерами использования `minGPT`.
- `tests/` — базовые проверки (например, импорт моделей HuggingFace).

## Лицензия

MIT
